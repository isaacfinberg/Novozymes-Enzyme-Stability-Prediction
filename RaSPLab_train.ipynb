{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RaSPLab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <b><font color='#009e74'>Rapid protein stability prediction using deep learning representations </font></b>\n",
        "\n",
        "Preprint pipeline version for predicting protein variants **thermodynamic stability changes** ($\\Delta \\Delta G$) using a deep learning representation. The program, using as input a protein structure (uploaded as PDB) returns stability predictions ($\\Delta \\Delta G$ in kcal/mol) for each variant at each position of the query protein.\n",
        "More details can be found in: **Blaabjerg et al.:** [\"Rapid protein stability prediction using deep learning representations\"](https://www.biorxiv.org/content/10.1101/2022.07.14.500157v1). Source code is available on the project [Github](https://github.com/KULL-Centre/papers/tree/main/2022/ML-ddG-Blaabjerg-et-al) page.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8szWTVF-dXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <b><font color='#009e74'> Reminders and Important informations:</font></b>\n",
        "- This notebook  <b><font color='#d55c00'>must</font></b> be run in a Colab GPU session (go to page menu: `Runtime`->  `Change runtime type` -> select `GPU` and confirm\n",
        "- Cells named as  <b><font color='#56b4e9'>PRELIMINARY OPERATIONS </font></b> have to be run <b><font color='#d55c00'>ONCE only at the start</font></b>  and  skipped for new predictions.\n",
        "- <b><font color='#d55c00'>ONE</font></b> single pdb at the time can be processed by the pipeline. \n",
        "- A  <b><font color='#d55c00'>new run</font></b> can be perform input direcly the new structure in the pdb upload cell and run the prediction cell again\n",
        "\n",
        "****"
      ],
      "metadata": {
        "id": "z2NZN9xv_2Gk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b><font color='#009e74'>PIPELINE : PRELIMINARY OPERATIONS </font></b>\n",
        "These cells should be run once at the start of the notebook\n",
        "****"
      ],
      "metadata": {
        "id": "I0tOqP4RNHbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><font color='#56b4e9'>PRELIMINARY OPERATIONS</font>: Setup enviroment and dependencies</b>\n",
        "\n",
        "#@markdown Run this cell to install the required enviroment and dependencies\n",
        "\n",
        "#@markdown **N.B: This cell should be run only ONCE at the START of the notebook.**\n",
        "%%bash\n",
        "rm -r sample_data\n",
        "\n",
        "#install minconda\n",
        "env PYTHONPATH= &> /dev/null\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-py37_4.9.2-Linux-x86_64.sh &> /dev/null\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT &> /dev/null\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT &> /dev/null\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX  &> /dev/null\n",
        "\n",
        "rm /content/Miniconda3-py37_4.9.2-Linux-x86_64.sh\n",
        "\n",
        "#update miniconda to latest 3.7\n",
        "conda install --channel defaults conda python=3.7 --yes &> /dev/null\n",
        "conda update --channel defaults --all --yes &> /dev/null\n",
        "\n",
        "echo \"-> Anaconda installed!\"\n",
        "\n",
        "#install dependencies\n",
        "\n",
        "# install dependencies present in pip\n",
        "pip install numpy==1.17.3 torch==1.2.0 biopython==1.72 matplotlib==3.1.1 pdb-tools &> /dev/null\n",
        "pip install --upgrade pdb-tools &> /dev/null\n",
        "\n",
        "# install remaining dependencies with conda\n",
        "conda install  mpl-scatter-density ptitprince pdbfixer=1.5 openmm=7.3.1 -c omnia -c conda-forge -c anaconda -c defaults --yes &> /dev/null\n",
        "\n",
        "echo \"--> Extra python packages installed!\"\n",
        "\n",
        "#install svn\n",
        "apt-get install -qq subversion &> /dev/null\n",
        "\n",
        "#mkdir of necessary folders\n",
        "mkdir data\n",
        "mkdir data/test\n",
        "mkdir data/test/predictions\n",
        "mkdir data/test/predictions/raw\n",
        "mkdir data/test/predictions/cleaned\n",
        "mkdir data/test/predictions/parsed\n",
        "mkdir output/\n",
        "mkdir output/predictions\n",
        "\n",
        "#download project folders from github\n",
        "\n",
        "svn checkout https://github.com/KULL-Centre/papers/trunk/2022/ML-ddG-Blaabjerg-et-al/src  &> /dev/null\n",
        "svn checkout https://github.com/KULL-Centre/papers/trunk/2022/ML-ddG-Blaabjerg-et-al/output/cavity_models  &> /dev/null\n",
        "svn checkout https://github.com/KULL-Centre/papers/trunk/2022/ML-ddG-Blaabjerg-et-al/output/ds_models  &> /dev/null\n",
        "\n",
        "wget -cq https://github.com/KULL-Centre/papers/raw/papers/2022/ML-ddG-Blaabjerg-et-al/data/pdb_frequencies.npz -o /content/data/pdb_frequencies.npz\n",
        "wget -cq https://github.com/KULL-Centre/papers/raw/main/2022/ML-ddG-Blaabjerg-et-al/colab_additonals/colab_additional.zip\n",
        "\n",
        "#extra files for runnin the notebooks\n",
        "\n",
        "mv ds_models ./output/\n",
        "mv cavity_models ./output/\n",
        "\n",
        "unzip colab_additional.zip &> /dev/null\n",
        "rm colab_additional.zip\n",
        "\n",
        "mv /content/colab_additional/best_model_path.txt /content/output/cavity_models/\n",
        "mv /content/colab_additional/clean_pdb.py /content/src/pdb_parser_scripts/\n",
        "mv /content/colab_additional/helpers.py /content/src/\n",
        "mv /content/colab_additional/pdb_frequencies.npz /content/data/\n",
        "\n",
        "echo \"---> Github data imported!\"\n",
        "\n",
        "#get and compile reduce\n",
        "\n",
        "cd src/pdb_parser_scripts\n",
        "git clone https://github.com/rlabduke/reduce.git\n",
        "cd reduce/\n",
        "make &> /dev/null\n",
        "\n",
        "mv /content/colab_additional/reduce /content/src/pdb_parser_scripts/reduce/\n",
        "\n",
        "chmod +x /content/src/pdb_parser_scripts/reduce/reduce \n",
        "echo \"----> reduce installed\"\n",
        "\n",
        "rm -r /content/colab_additional\n",
        "#@markdown ****"
      ],
      "metadata": {
        "id": "NTRMe6SmnAW0",
        "collapsed": true,
        "outputId": "d075b58a-d909-4125-8f5b-9d1016bb9042",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> Anaconda installed!\n",
            "--> Extra python packages installed!\n",
            "---> Github data imported!\n",
            "----> reduce installed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'reduce'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><font color='#56b4e9'>PRELIMINARY OPERATIONS</font>: Import python libraries, functions and setup common variables</b>\n",
        "\n",
        "#@markdown Run this cell to import libraries and functions necessary for the pipeline.\n",
        "\n",
        "#@markdown **N.B: This cell should be run only ONCE at the START of the notebook.**\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/usr/local/lib/python3.7/site-packages\")\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import pathlib\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "import datetime\n",
        "import matplotlib\n",
        "from pdbfixer import PDBFixer\n",
        "from simtk.openmm.app import PDBFile\n",
        "from Bio.PDB.Polypeptide import index_to_one, one_to_index\n",
        "from collections import OrderedDict\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from google.colab import files\n",
        "\n",
        "sys.path.append('./src/')\n",
        "\n",
        "from cavity_model import (\n",
        "     CavityModel,\n",
        "     DownstreamModel,\n",
        "     ResidueEnvironment,\n",
        "     ResidueEnvironmentsDataset,\n",
        ")\n",
        "\n",
        "from helpers import (\n",
        "     populate_dfs_with_resenvs,\n",
        "     remove_disulfides,\n",
        "     fermi_transform,\n",
        "     inverse_fermi_transform,\n",
        "     init_lin_weights,\n",
        "     ds_pred,\n",
        "     cavity_to_prism,\n",
        "     get_seq_from_variant,\n",
        ")\n",
        "\n",
        "from visualization import (\n",
        "     hist_plot,\n",
        ")\n",
        "\n",
        "#Extra function to fix pdb\n",
        "\n",
        "# Setup pipeline parameters\n",
        "## Set seeds\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "## Main deep parameters\n",
        "DEVICE = \"cuda\"  # \"cpu\" or \"cuda\"\n",
        "NUM_ENSEMBLE = 10\n",
        "TASK_ID = int(1)\n",
        "PER_TASK = int(1)\n",
        "\n",
        "#@markdown ****"
      ],
      "metadata": {
        "id": "QKz_yq3VxDCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fz0eLaf_jeux",
        "outputId": "c3d16d09-c07b-454e-a9d3-b204f5d56e8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyrosettacolabsetup\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "# levinthal / paradox\n",
        "import pyrosettacolabsetup; pyrosettacolabsetup.install_pyrosetta()\n",
        "import pyrosetta; pyrosetta.init()\n",
        "import pyrosetta\n",
        "pyrosetta.init()\n",
        "from pyrosetta.teaching import *"
      ],
      "metadata": {
        "id": "1ScFFW_zmWaW",
        "outputId": "8c322292-8adb-4c83-fa0d-3c01a0e42847",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyrosettacolabsetup\n",
            "  Downloading pyrosettacolabsetup-1.0.6-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: pyrosettacolabsetup\n",
            "Successfully installed pyrosettacolabsetup-1.0.6\n",
            "Mounted at /content/google_drive\n",
            "Looking for compatible PyRosetta wheel file at google-drive/PyRosetta/colab.bin/wheels...\n",
            "Found compatible wheel: /content/google_drive/MyDrive/PyRosetta/colab.bin/wheels//content/google_drive/MyDrive/PyRosetta/colab.bin/wheels/pyrosetta-2022.47+release.d2aee95a6b7-cp37-cp37m-linux_x86_64.whl\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\n",
            "\n",
            "PyRosetta-4 2022 [Rosetta PyRosetta4.MinSizeRel.python37.ubuntu 2022.47+release.d2aee95a6b7bf6ee70c5e2c7b29d0915e9112fa7 2022-11-23T13:33:36] retrieved from: http://www.pyrosetta.org\n",
            "(C) Copyright Rosetta Commons Member Institutions. Created in JHU by Sergey Lyskov and PyRosetta Team.\n",
            "core.init: Checking for fconfig files in pwd and ./rosetta/flags\n",
            "core.init: Rosetta version: PyRosetta4.MinSizeRel.python37.ubuntu r336 2022.47+release.d2aee95a6b7 d2aee95a6b7bf6ee70c5e2c7b29d0915e9112fa7 http://www.pyrosetta.org 2022-11-23T13:33:36\n",
            "core.init: command: PyRosetta -ex1 -ex2aro -database /usr/local/lib/python3.7/dist-packages/pyrosetta/database\n",
            "basic.random.init_random_generator: 'RNG device' seed mode, using '/dev/urandom', seed=607127015 seed_offset=0 real_seed=607127015\n",
            "basic.random.init_random_generator: RandomGenerator:init: Normal mode, seed=607127015 RG_type=mt19937\n",
            "PyRosetta-4 2022 [Rosetta PyRosetta4.MinSizeRel.python37.ubuntu 2022.47+release.d2aee95a6b7bf6ee70c5e2c7b29d0915e9112fa7 2022-11-23T13:33:36] retrieved from: http://www.pyrosetta.org\n",
            "(C) Copyright Rosetta Commons Member Institutions. Created in JHU by Sergey Lyskov and PyRosetta Team.\n",
            "core.init: Checking for fconfig files in pwd and ./rosetta/flags\n",
            "core.init: Rosetta version: PyRosetta4.MinSizeRel.python37.ubuntu r336 2022.47+release.d2aee95a6b7 d2aee95a6b7bf6ee70c5e2c7b29d0915e9112fa7 http://www.pyrosetta.org 2022-11-23T13:33:36\n",
            "core.init: command: PyRosetta -ex1 -ex2aro -database /usr/local/lib/python3.7/dist-packages/pyrosetta/database\n",
            "basic.random.init_random_generator: 'RNG device' seed mode, using '/dev/urandom', seed=-2040161364 seed_offset=0 real_seed=-2040161364\n",
            "basic.random.init_random_generator: RandomGenerator:init: Normal mode, seed=-2040161364 RG_type=mt19937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyrosetta.rosetta.protocols.ddg.CartesianddG import MutationSet"
      ],
      "metadata": {
        "id": "sm0qbV_uosi8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdb_path = '/content/google_drive/MyDrive/kaggle_deep_learning/final_project/pdbs/'\n",
        "mut = [pdb_path + i for i in os.listdir(pdb_path) if i.endswith('.pdb')][0]\n",
        "wt = '/content/google_drive/MyDrive/kaggle_deep_learning/final_project/cd1a.pdb'\n",
        "wt_pose = pyrosetta.pose_from_pdb(wt)\n",
        "mt_pose = pyrosetta.pose_from_pdb(mut)\n",
        "mt_set = MutationSet([17], ['A'], 3)\n",
        "# mt_set.add_wildtypes(wt_pose)\n",
        "# pyrosetta.rosetta.protocols.ddg.CartesianddG.run("
      ],
      "metadata": {
        "id": "HXSffrdWpFl7",
        "outputId": "c7a3f4f3-fe62-4aa9-f3f9-53fb2a044ff8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "core.import_pose.import_pose: File '/content/google_drive/MyDrive/kaggle_deep_learning/final_project/cd1a.pdb' automatically determined to be of type PDB\n",
            "core.conformation.Conformation: [ WARNING ] missing heavyatom:  OXT on residue LYS:CtermProteinFull 221\n",
            "core.conformation.Conformation: Found disulfide between residues 47 177\n",
            "core.conformation.Conformation: current variant for 47 CYS\n",
            "core.conformation.Conformation: current variant for 177 CYS\n",
            "core.conformation.Conformation: current variant for 47 CYD\n",
            "core.conformation.Conformation: current variant for 177 CYD\n",
            "core.conformation.Conformation: Found disulfide between residues 52 188\n",
            "core.conformation.Conformation: current variant for 52 CYS\n",
            "core.conformation.Conformation: current variant for 188 CYS\n",
            "core.conformation.Conformation: current variant for 52 CYD\n",
            "core.conformation.Conformation: current variant for 188 CYD\n",
            "core.import_pose.import_pose: File '/content/google_drive/MyDrive/kaggle_deep_learning/final_project/pdbs/33303.pdb' automatically determined to be of type PDB\n",
            "core.conformation.Conformation: [ WARNING ] missing heavyatom:  OXT on residue LYS:CtermProteinFull 221\n",
            "core.conformation.Conformation: Found disulfide between residues 47 177\n",
            "core.conformation.Conformation: current variant for 47 CYS\n",
            "core.conformation.Conformation: current variant for 177 CYS\n",
            "core.conformation.Conformation: current variant for 47 CYD\n",
            "core.conformation.Conformation: current variant for 177 CYD\n",
            "core.conformation.Conformation: Found disulfide between residues 52 188\n",
            "core.conformation.Conformation: current variant for 52 CYS\n",
            "core.conformation.Conformation: current variant for 188 CYS\n",
            "core.conformation.Conformation: current variant for 52 CYD\n",
            "core.conformation.Conformation: current variant for 188 CYD\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5b1ebb6c1134>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwt_pose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyrosetta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_from_pdb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmt_pose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyrosetta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_from_pdb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmt_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMutationSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# mt_set.add_wildtypes(wt_pose)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# pyrosetta.rosetta.protocols.ddg.CartesianddG.run(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__(): incompatible constructor arguments. The following argument types are supported:\n    1. pyrosetta.rosetta.protocols.ddg.CartesianddG.MutationSet(resnums: pyrosetta.rosetta.utility.vector1_unsigned_long, mutations: pyrosetta.rosetta.utility.vector1_core_chemical_AA, iterations: int)\n    2. pyrosetta.rosetta.protocols.ddg.CartesianddG.MutationSet(arg0: pyrosetta.rosetta.protocols.ddg.CartesianddG.MutationSet)\n\nInvoked with: [17], ['A'], 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pyrosetta.rosetta.utility.vector1_unsigned_long([])"
      ],
      "metadata": {
        "id": "S3eAhE7fx283",
        "outputId": "d098d757-e91e-4591-a092-386740c4c54e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-63956483f9ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpyrosetta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrosetta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector1_unsigned_long\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: __init__(): incompatible constructor arguments. The following argument types are supported:\n    1. pyrosetta.rosetta.utility.vector1_unsigned_long()\n    2. pyrosetta.rosetta.utility.vector1_unsigned_long(arg0: int)\n    3. pyrosetta.rosetta.utility.vector1_unsigned_long(arg0: pyrosetta.rosetta.utility.vector1_unsigned_long)\n\nInvoked with: [0]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.cavity_model import *\n",
        "\n",
        "pdbs = os.listdir('/content/drive/MyDrive/kaggle_deep_learning/final_project/pdbs/')\n",
        "pdbs = [i for i in pdbs if i.endswith('.pdb')]\n",
        "for pdb in pdbs[:5]:\n",
        "  in_clean = '/content/drive/MyDrive/kaggle_deep_learning/final_project/pdbs/' + pdb\n",
        "  out_clean = '/content/drive/MyDrive/kaggle_deep_learning/final_project/cleaned/'\n",
        "  in_parse = '/content/drive/MyDrive/kaggle_deep_learning/final_project/cleaned/' + pdb.split('.')[0] + '_clean.pdb'\n",
        "  out_parse = '/content/drive/MyDrive/kaggle_deep_learning/final_project/parsed/'\n",
        "  !python3 /content/src/pdb_parser_scripts/clean_pdb.py --pdb_file_in \"$in_clean\" --out_dir \"$out_clean\" --reduce_exe /content/src/pdb_parser_scripts/reduce/reduce #&> /dev/null\n",
        "  !python3 /content/src/pdb_parser_scripts/extract_environments.py --pdb_in \"$in_parse\" --out_dir \"$out_parse\"\n"
      ],
      "metadata": {
        "id": "pXXXwGJahWUK",
        "outputId": "faf5d958-c690-4a6e-b667-f4c04bdd842e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: A  18 LYS NZ   H atom too close to  A  18 LYS CE   by -0.15265A\n",
            "        if warning inappropriate, adjust the cuttoffs using -METALBump or -NONMETALBump.\n",
            "WARNING: A  18 LYS NZ   H atom too close to  A 146 ASN CA   by -0.258144A\n",
            "        if warning inappropriate, adjust the cuttoffs using -METALBump or -NONMETALBump.\n",
            "WARNING: A  18 LYS NZ   H atom too close to  A  18 LYS CE   by -0.15265A\n",
            "        if warning inappropriate, adjust the cuttoffs using -METALBump or -NONMETALBump.\n",
            "WARNING: A  18 LYS NZ   H atom too close to  A  18 LYS CE   by -0.15265A\n",
            "        if warning inappropriate, adjust the cuttoffs using -METALBump or -NONMETALBump.\n",
            "Time for cleaning /content/drive/MyDrive/kaggle_deep_learning/final_project/pdbs/33303.pdb: 6.872405052185059\n",
            "/usr/local/lib/python3.7/site-packages/Bio/PDB/Vector.py:42: BiopythonDeprecationWarning: The module Bio.PDB.Vector has been deprecated in favor of new module Bio.PDB.vectors to solve a name collision with the class Vector. For the class Vector, and vector functions like calc_angle, import from Bio.PDB instead.\n",
            "  \"import from Bio.PDB instead.\", BiopythonDeprecationWarning)\n",
            "/content/src/pdb_parser_scripts/extract_environments.py:109: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  features[\"res_indices\"] = np.array(features[\"res_indices\"], dtype=np.int)\n",
            "Time for parsing environments from /content/drive/MyDrive/kaggle_deep_learning/final_project/cleaned/33303_clean.pdb: 0.34349966049194336\n",
            "Time for cleaning /content/drive/MyDrive/kaggle_deep_learning/final_project/pdbs/33304.pdb: 4.426265001296997\n",
            "/usr/local/lib/python3.7/site-packages/Bio/PDB/Vector.py:42: BiopythonDeprecationWarning: The module Bio.PDB.Vector has been deprecated in favor of new module Bio.PDB.vectors to solve a name collision with the class Vector. For the class Vector, and vector functions like calc_angle, import from Bio.PDB instead.\n",
            "  \"import from Bio.PDB instead.\", BiopythonDeprecationWarning)\n",
            "/content/src/pdb_parser_scripts/extract_environments.py:109: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  features[\"res_indices\"] = np.array(features[\"res_indices\"], dtype=np.int)\n",
            "Time for parsing environments from /content/drive/MyDrive/kaggle_deep_learning/final_project/cleaned/33304_clean.pdb: 0.3484482765197754\n",
            "Time for cleaning /content/drive/MyDrive/kaggle_deep_learning/final_project/pdbs/33305.pdb: 4.436565637588501\n",
            "/usr/local/lib/python3.7/site-packages/Bio/PDB/Vector.py:42: BiopythonDeprecationWarning: The module Bio.PDB.Vector has been deprecated in favor of new module Bio.PDB.vectors to solve a name collision with the class Vector. For the class Vector, and vector functions like calc_angle, import from Bio.PDB instead.\n",
            "  \"import from Bio.PDB instead.\", BiopythonDeprecationWarning)\n",
            "/content/src/pdb_parser_scripts/extract_environments.py:109: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  features[\"res_indices\"] = np.array(features[\"res_indices\"], dtype=np.int)\n",
            "Time for parsing environments from /content/drive/MyDrive/kaggle_deep_learning/final_project/cleaned/33305_clean.pdb: 0.3314502239227295\n",
            "WARNING: A  18 LYS NZ   H atom too close to  A  18 LYS CE   by -0.15192A\n",
            "        if warning inappropriate, adjust the cuttoffs using -METALBump or -NONMETALBump.\n",
            "WARNING: A  18 LYS NZ   H atom too close to  A 146 ASN C    by -0.109367A\n",
            "        if warning inappropriate, adjust the cuttoffs using -METALBump or -NONMETALBump.\n",
            "WARNING: A  18 LYS NZ   H atom too close to  A  18 LYS CE   by -0.15192A\n",
            "        if warning inappropriate, adjust the cuttoffs using -METALBump or -NONMETALBump.\n",
            "WARNING: A  18 LYS NZ   H atom too close to  A 146 ASN CA   by -0.193258A\n",
            "        if warning inappropriate, adjust the cuttoffs using -METALBump or -NONMETALBump.\n",
            "WARNING: A  18 LYS NZ   H atom too close to  A 146 ASN C    by -0.109367A\n",
            "        if warning inappropriate, adjust the cuttoffs using -METALBump or -NONMETALBump.\n",
            "WARNING: A  18 LYS NZ   H atom too close to  A  18 LYS CE   by -0.15192A\n",
            "        if warning inappropriate, adjust the cuttoffs using -METALBump or -NONMETALBump.\n",
            "WARNING: A  18 LYS NZ   H atom too close to  A  18 LYS CE   by -0.15192A\n",
            "        if warning inappropriate, adjust the cuttoffs using -METALBump or -NONMETALBump.\n",
            "Time for cleaning /content/drive/MyDrive/kaggle_deep_learning/final_project/pdbs/33306.pdb: 4.406693935394287\n",
            "/usr/local/lib/python3.7/site-packages/Bio/PDB/Vector.py:42: BiopythonDeprecationWarning: The module Bio.PDB.Vector has been deprecated in favor of new module Bio.PDB.vectors to solve a name collision with the class Vector. For the class Vector, and vector functions like calc_angle, import from Bio.PDB instead.\n",
            "  \"import from Bio.PDB instead.\", BiopythonDeprecationWarning)\n",
            "/content/src/pdb_parser_scripts/extract_environments.py:109: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  features[\"res_indices\"] = np.array(features[\"res_indices\"], dtype=np.int)\n",
            "Time for parsing environments from /content/drive/MyDrive/kaggle_deep_learning/final_project/cleaned/33306_clean.pdb: 0.32717442512512207\n",
            "Time for cleaning /content/drive/MyDrive/kaggle_deep_learning/final_project/pdbs/33307.pdb: 4.351994037628174\n",
            "/usr/local/lib/python3.7/site-packages/Bio/PDB/Vector.py:42: BiopythonDeprecationWarning: The module Bio.PDB.Vector has been deprecated in favor of new module Bio.PDB.vectors to solve a name collision with the class Vector. For the class Vector, and vector functions like calc_angle, import from Bio.PDB instead.\n",
            "  \"import from Bio.PDB instead.\", BiopythonDeprecationWarning)\n",
            "/content/src/pdb_parser_scripts/extract_environments.py:109: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  features[\"res_indices\"] = np.array(features[\"res_indices\"], dtype=np.int)\n",
            "Time for parsing environments from /content/drive/MyDrive/kaggle_deep_learning/final_project/cleaned/33307_clean.pdb: 0.3623831272125244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.helpers import *\n",
        "\n",
        "parsed_pdb_filenames = os.listdir(out_parse)\n",
        "parsed_pdb_filenames = [out_parse + pdb for pdb in parsed_pdb_filenames]\n",
        "BATCH_SIZE = 100\n",
        "TRAIN_VAL_SPLIT = 0.77\n",
        "DEVICE = \"cuda\"\n",
        "populate_dfs_with_resenvs(ddg_data, resenv_dataset)\n",
        "dataloader_train = get_ddg_dataloader(ddg_data, 'train', BATCH_SIZE, DEVICE)\n",
        "dataloader_val = get_ddg_dataloader(ddg_data, 'val', BATCH_SIZE, DEVICE)\n",
        "df_ddg = None\n",
        "best_cavity_model_path = open(f\"/content/output/cavity_models/best_model_path.txt\", \"r\").read()\n",
        "cavity_model_net = CavityModel(get_latent=True).to(DEVICE)\n",
        "cavity_model_net.load_state_dict(torch.load(f\"{best_cavity_model_path}\"))\n",
        "loss_func = torch.nn.L1Loss()\n",
        "EPOCHS = 1\n",
        "for idx in [range(NUM_ENSEMBLE)[0]]:\n",
        "  ds_model_net = DownstreamModel().to(DEVICE)\n",
        "  ds_model_net.load_state_dict(\n",
        "                  torch.load(\n",
        "                      f\"/content/output/ds_models/ds_model_{idx}/model.pt\"\n",
        "                  )\n",
        "              )\n",
        "  optimizer = torch.optim.Adam(ds_model_net.parameters(), lr=0.0003)\n",
        "  model_idx = idx + NUM_ENSEMBLE\n",
        "  ds_train_val(\n",
        "      df_ddg,\n",
        "      dataloader_train,\n",
        "      dataloader_val,\n",
        "      cavity_model_net,\n",
        "      ds_model_net,\n",
        "      loss_func,\n",
        "      optimizer,\n",
        "      model_idx,\n",
        "      EPOCHS,\n",
        "      DEVICE,\n",
        "  )"
      ],
      "metadata": {
        "id": "Mqg3Kv2CsI5Z",
        "outputId": "0734ed39-9512-4771-cbc3-7c72401aef6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data set includes 3 pdbs with 663 environments.\n",
            "Validation data set includes 2 pdbs with 442 environments.\n",
            "Epoch: 1/1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4e8da7e631e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mmodel_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m   )\n",
            "\u001b[0;32m/content/src/helpers.py\u001b[0m in \u001b[0;36mds_train_val\u001b[0;34m(df_ddg, dataloader_train, dataloader_val, cavity_model_net, ds_model_net, loss_func, optimizer, model_idx, EPOCHS, DEVICE)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mds_model_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cavity_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ds_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddg_fermi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0;31m# Initialize optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b><font color='#009e74'>PIPELINE : PREDICTIONS </font></b>"
      ],
      "metadata": {
        "id": "OpUMk4UezkLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><font color='#56b4e9'> PDB upload</font></b>\n",
        "\n",
        "#@markdown Choose between <b><font color='#d55c00'> ONE</font></b> of the possible input sources for the target pdb and <ins>leave the other cells empty or unmarked</ins>\n",
        "#@markdown - AlphaFold2 PDB via Uniprot ID:\n",
        "AF_ID ='P04036'#@param {type:\"string\"}\n",
        "#@markdown - PDB ID (imported from RCSB PDB):\n",
        "PDB_ID =''#@param {type:\"string\"}\n",
        "#@markdown - Upload custom PDB\n",
        "PDB_custom =False#@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown Select target chain (default A)\n",
        "chain='A' #@param {type:'string'}\n",
        "\n",
        "if os.path.exists(\"/content/query_protein.pdb\"):\n",
        "    os.remove(\"/content/query_protein.pdb\")\n",
        "if os.path.exists(\"/content/data/test/predictions/raw/query_protein_uniquechain.pdb\"):\n",
        "    os.remove(\"/content/data/test/predictions/raw/query_protein_uniquechain.pdb\")\n",
        "if os.path.exists(\"/content/data/test/predictions/cleaned/query_protein_uniquechain_clean.pdb\"):\n",
        "    os.remove(\"/content/data/test/predictions/cleaned/query_protein_uniquechain_clean.pdb\")\n",
        "if os.path.exists(\"/content/data/test/predictions/parsed/query_protein_uniquechain_clean_coordinate_features.npz\"):\n",
        "    os.remove(\"/content/data/test/predictions/parsed/query_protein_uniquechain_clean_coordinate_features.npz\")\n",
        "\n",
        "if (AF_ID !='') and (len(AF_ID)==6) : \n",
        "    subprocess.call(['curl','-s','-f',f'https://alphafold.ebi.ac.uk/files/AF-{AF_ID}-F1-model_v2.pdb','-o','/content/query_protein.pdb'])\n",
        "elif (PDB_ID !='') and (len(PDB_ID)==4):\n",
        "    subprocess.call(['curl','-s','-f',f'https://files.rcsb.org/download/{PDB_ID}.pdb','-o','/content/query_protein.pdb'])\n",
        "\n",
        "elif PDB_custom:\n",
        "  print('Upload PDB file:')\n",
        "  uploaded_pdb = files.upload()\n",
        "  for fn in uploaded_pdb.keys():\n",
        "    os.rename(fn, f\"/content/query_protein.pdb\")\n",
        "    print('PDB file correctly loaded')\n",
        "\n",
        "else:\n",
        "  print(f'ERROR: any PDB uploaded, please select one of the above inputs')\n",
        "\n",
        "\n",
        "\n",
        "#@markdown N.B. This cell will also perform preliminary operations to correcly format the uploaded PDB\n",
        "\n",
        "## remove other chains and move to raw folder\n",
        "!pdb_selchain -\"$chain\" /content/query_protein.pdb | pdb_delhetatm | pdb_delres --999:0:1 | pdb_fixinsert | pdb_tidy  > /content/data/test/predictions/raw/query_protein_uniquechain.pdb\n",
        "# Select PDBs to run during this task - could be simplified if we decide to set PER_TASK = 1 for all cases\n",
        "\n",
        "pdb_input_dir = \"data/test/predictions/raw/\"\n",
        "input_pdbs = sorted(list(filter(lambda x: x.endswith(\".pdb\"), os.listdir('data/test/predictions/raw/'))))\n",
        "start = (TASK_ID-1)*(PER_TASK)\n",
        "end = (TASK_ID*PER_TASK)\n",
        "if end > len(input_pdbs):\n",
        "    end = len(input_pdbs) #avoid end index exceeding length of list\n",
        "pdbs = input_pdbs[start:end] \n",
        "pdb_names = [i.split(\".\")[0] for i in pdbs]\n",
        "print(pdb_names)\n",
        "print(f\"Pre-processing PDBs ...\")\n",
        "\n",
        "!python3 /content/src/pdb_parser_scripts/clean_pdb.py --pdb_file_in /content/data/test/predictions/raw/query_protein_uniquechain.pdb --out_dir /content/data/test/predictions/cleaned/ --reduce_exe /content/src/pdb_parser_scripts/reduce/reduce #&> /dev/null\n",
        "!python3 /content/src/pdb_parser_scripts/extract_environments.py --pdb_in /content/data/test/predictions/cleaned/query_protein_uniquechain_clean.pdb  --out_dir /content/data/test/predictions/parsed/  #&> /dev/null\n",
        "\n",
        "if os.path.exists(\"/content/data/test/predictions/parsed/query_protein_uniquechain_clean_coordinate_features.npz\"):\n",
        "  print(f\"Pre-processing PDBs correctly ended\")\n",
        "else:\n",
        "  print(f\"Pre-processing PDB didn't end correcly, please check input informations\")\n",
        "\n",
        "#@markdown ****"
      ],
      "metadata": {
        "id": "Z8nUmHI5rgjy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title <b><font color='#56b4e9'> Pipeline RUN </font></b>\n",
        "\n",
        "#@markdown <b><font color='#d55c00'>Execute the cell</font></b> to run the pipeline and generate **saturation mutagenesis predictions of thermodynamic stability changes** predictions\n",
        "\n",
        "### Pre-process structure data\n",
        "\n",
        "# Create temporary residue environment datasets to more easily match ddG data\n",
        "pdb_filenames_ds = sorted(glob.glob(f\"/content/data/test/predictions/parsed/*coord*\"))\n",
        "\n",
        "dataset_structure = ResidueEnvironmentsDataset(pdb_filenames_ds, transformer=None)\n",
        "\n",
        "resenv_dataset = {}\n",
        "for resenv in dataset_structure:\n",
        "    if AF_ID!='':\n",
        "      key = (f\"--{AF_ID}--{resenv.chain_id}--{resenv.pdb_residue_number}--{index_to_one(resenv.restype_index)}--\"\n",
        "          )\n",
        "    elif PDB_ID!='':\n",
        "      key = (f\"--{PDB_ID}--{resenv.chain_id}--{resenv.pdb_residue_number}--{index_to_one(resenv.restype_index)}--\"\n",
        "          )\n",
        "    else:\n",
        "      key = (f\"--{'CUSTOM'}--{resenv.chain_id}--{resenv.pdb_residue_number}--{index_to_one(resenv.restype_index)}--\"\n",
        "          )\n",
        "    resenv_dataset[key] = resenv\n",
        "df_structure_no_mt = pd.DataFrame.from_dict(resenv_dataset, orient='index', columns=[\"resenv\"])\n",
        "df_structure_no_mt.reset_index(inplace=True)\n",
        "df_structure_no_mt[\"index\"]=df_structure_no_mt[\"index\"].astype(str)\n",
        "res_info = pd.DataFrame(df_structure_no_mt[\"index\"].str.split('--').tolist(),\n",
        "                        columns = ['blank','pdb_id','chain_id','pos','wt_AA', 'blank2'])\n",
        "\n",
        "df_structure_no_mt[\"pdbid\"] = res_info['pdb_id']\n",
        "df_structure_no_mt[\"chainid\"] = res_info['chain_id']\n",
        "df_structure_no_mt[\"variant\"] = res_info[\"wt_AA\"] + res_info['pos'] + \"X\"\n",
        "aa_list = [\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \n",
        "            \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"]\n",
        "df_structure = pd.DataFrame(df_structure_no_mt.values.repeat(20, axis=0), columns=df_structure_no_mt.columns)\n",
        "for i in range(0, len(df_structure), 20):\n",
        "    for j in range(20):\n",
        "        df_structure.iloc[i+j, :][\"variant\"] = df_structure.iloc[i+j, :][\"variant\"][:-1] + aa_list[j]\n",
        "df_structure.drop(columns=\"index\", inplace=True)\n",
        "\n",
        "# Load PDB amino acid frequencies used to approximate unfolded states\n",
        "pdb_nlfs = -np.log(np.load(f\"{os.getcwd()}/data/pdb_frequencies.npz\")[\"frequencies\"])\n",
        "\n",
        "# # Add wt and mt idxs and freqs to df\n",
        "\n",
        "df_structure[\"wt_idx\"] = df_structure.apply(lambda row: one_to_index(row[\"variant\"][0]), axis=1)\n",
        "df_structure[\"mt_idx\"] = df_structure.apply(lambda row: one_to_index(row[\"variant\"][-1]), axis=1)\n",
        "df_structure[\"wt_nlf\"] = df_structure.apply(lambda row: pdb_nlfs[row[\"wt_idx\"]], axis=1)\n",
        "df_structure[\"mt_nlf\"] = df_structure.apply(lambda row: pdb_nlfs[row[\"mt_idx\"]], axis=1)\n",
        "\n",
        "# Define models\n",
        "best_cavity_model_path = open(f\"/content/output/cavity_models/best_model_path.txt\", \"r\").read()\n",
        "cavity_model_net = CavityModel(get_latent=True).to(DEVICE)\n",
        "cavity_model_net.load_state_dict(torch.load(f\"{best_cavity_model_path}\"))\n",
        "cavity_model_net.eval()\n",
        "ds_model_net = DownstreamModel().to(DEVICE)\n",
        "ds_model_net.apply(init_lin_weights)\n",
        "ds_model_net.eval()\n",
        "\n",
        "###set start time\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "# Make ML predictions\n",
        "print(f\"Starting downstream model prediction\")\n",
        "dataset_key=\"predictions\"\n",
        "df_ml = ds_pred(cavity_model_net,\n",
        "                ds_model_net,\n",
        "                df_structure,\n",
        "                dataset_key,\n",
        "                NUM_ENSEMBLE,\n",
        "                DEVICE,\n",
        "                ) \n",
        "print(f\"Finished downstream model prediction\")\n",
        "end_time = time.perf_counter()\n",
        "elapsed = datetime.timedelta(seconds = end_time - start_time)\n",
        "print(\"Complete - prediction execution took\", elapsed)\n",
        "\n",
        "elapsed = datetime.timedelta(seconds = end_time - start_time)\n",
        "print(\"Generating output files\")\n",
        "#Merge and save data with predictions\n",
        "\n",
        "df_total = df_structure.merge(df_ml, on=['pdbid','chainid','variant'], how='outer')\n",
        "#df_total[\"b_factors\"] = df_total.apply(lambda row: row[\"resenv\"].b_factors, axis=1)\n",
        "df_total = df_total.drop(\"resenv\", 1)\n",
        "print(f\"{len(df_structure)-len(df_ml)} data points dropped when matching total data with ml predictions in: {dataset_key}.\")\n",
        "\n",
        "# Parse output into separate files by pdb, print to PRISM format\n",
        "for pdbid in df_total[\"pdbid\"].unique():\n",
        "    df_pdb = df_total[df_total[\"pdbid\"]==pdbid]\n",
        "    for chainid in df_pdb[\"chainid\"].unique():\n",
        "        pred_outfile = f\"{os.getcwd()}/output/{dataset_key}/cavity_pred_{pdbid}_{chainid}.csv\"\n",
        "        print(f\"Parsing predictions from pdb: {pdbid}{chainid} into {pred_outfile}\")\n",
        "        df_chain = df_pdb[df_pdb[\"chainid\"]==chainid]\n",
        "        df_chain = df_chain.assign(pos = df_chain[\"variant\"].str[1:-1])\n",
        "        df_chain['pos'] = pd.to_numeric(df_chain['pos'])\n",
        "        first_res_no = min(df_chain[\"pos\"])\n",
        "        df_chain = df_chain.assign(wt_AA = df_chain[\"variant\"].str[0])\n",
        "        df_chain = df_chain.assign(mt_AA = df_chain[\"variant\"].str[-1])\n",
        "        seq = get_seq_from_variant(df_chain)\n",
        "        df_chain.to_csv(pred_outfile, index=False)\n",
        "        prism_outfile = f\"/content/output/{dataset_key}/prism_cavity_{pdbid}_{chainid}.txt\"\n",
        "\n",
        "        # if (AF_ID !=''):\n",
        "        #   prism_outfile = f\"/content/output/{dataset_key}/prism_cavity_{AF_ID}_{chainid}.txt\"\n",
        "        # elif (PDB_ID !=''):\n",
        "        #   prism_outfile = f\"/content/output/{dataset_key}/prism_cavity_{PDB_ID}_{chainid}.txt\"\n",
        "        # elif PDB_custom:\n",
        "        #   prism_outfile = f\"/content/output/{dataset_key}/prism_cavity_XXXX_{chainid}.txt\"\n",
        "        cavity_to_prism(df_chain, pdbid, chainid, seq, prism_outfile)\n",
        "\n",
        "# End timer and print result\n",
        "#!rm /content/output/predictions/*xxxx*.csv\n",
        "elapsed = datetime.timedelta(seconds = end_time - start_time)\n",
        "print(\"Complete - files generated\")\n",
        "\n",
        "#@markdown ****"
      ],
      "metadata": {
        "id": "tAxW8XNqxCS_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><font color='#56b4e9'> Download results as archive </font></b>\n",
        "\n",
        "#@markdown Run the cell to <b><font color='#009e74'> download a .zip archive </font></b> with prediction files for the <ins>current run</ins>.\n",
        "\n",
        "#@markdown <ins>Tick</ins> the next box if you ran multiple predictions and you want to <ins>download all of them</ins>.\n",
        "\n",
        "download_all_predictions= False #@param {type:\"boolean\"}\n",
        "\n",
        "if download_all_predictions:\n",
        "  os.system( \"zip -r {} {}\".format( f\"predictions_output_all.zip\" , f\"/content/output/predictions/*\" ) )\n",
        "  files.download(f\"predictions_output_all.zip\")\n",
        "else:\n",
        "  if (AF_ID !=''):\n",
        "    os.system( \"zip -r {} {}\".format( f\"predictions_output_{AF_ID}.zip\" , f\"/content/output/predictions/*{AF_ID}*\" ) )\n",
        "    files.download(f\"predictions_output_{AF_ID}.zip\")\n",
        "  elif (PDB_ID !=''):\n",
        "    os.system( \"zip -r {} {}\".format( f\"predictions_output_{PDB_ID}.zip\" , f\"/content/output/predictions/*{PDB_ID}*\" ) )\n",
        "    files.download(f\"predictions_output_{PDB_ID}.zip\")\n",
        "  else:\n",
        "    os.system( \"zip -r {} {}\".format( f\"predictions_output.zip\" , f\"/content/output/predictions\" ) )\n",
        "    files.download(f\"predictions_output.zip\")\n",
        "\n",
        "  if download_all_predictions:\n",
        "    os.system( \"zip -r {} {}\".format( f\"predictions_output.zip\" , f\"/content/output/predictions\" ) )\n",
        "    files.download(f\"predictions_output_all.zip\")\n",
        "\n",
        "#@markdown **P.S.: prediction files are also stored in the colab file system folder: `/output/predictions/`**\n",
        "#@markdown ****"
      ],
      "metadata": {
        "id": "tVAoMFcNlasI",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Troubleshooting**\n",
        "\n",
        "- Check that the runtime type is set to GPU at \"Runtime\" -> \"Change runtime type\".\n",
        "- Try to restart the session \"Runtime\" -> \"Factory reset runtime\".\n",
        "- Check your input pdb.\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Known problems:**\n",
        "\n",
        "- Residues with numeration index below 0 are not supported by the output file parser and thus they deleted from the pdb in the pre-processing step.\n",
        "- Insertion annotations in the pdb are not supported. Any annotations is actually deleted during the pre-processing step.\n",
        "\n",
        "\\\\\n",
        "\n",
        "**License:**\n",
        "\n",
        "RaSP's source code is licensed under the permissive Apache Licence, Version 2.0.\n",
        " Additionally, this notebook uses the reduce source code which license could be find in `/content/src/pdb_parser_scripts/reduce/`\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Bugs:**\n",
        "\n",
        "For any bugs please report the issue on the project [Github](https://github.com/KULL-Centre/papers/tree/main/2022/ML-ddG-Blaabjerg-et-al) or contact one of the listed authors in the connected [manuscript](https://www.biorxiv.org/content/10.1101/2022.07.14.500157v1).\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Citing this work:**\n",
        "\n",
        "If you use our model please cite:\n",
        "\n",
        "Blaabjerg, L.M., Kassem, M.M., Good, L.L., Jonsson, N., Cagiada, M., Johansson, K.E., Boomsma, W., Stein, A. and Lindorff-Larsen, K., 2022. *Rapid protein stability prediction using deep learning representations*, bioRxiv. (https://doi.org/10.1101/2022.07.14.500157)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "@article{blaabjerg2022rapid,\n",
        "  title={Rapid protein stability prediction using deep learning representations},\n",
        "  author={Blaabjerg, Lasse M and Kassem, Maher M and Good, Lydia L and Jonsson, Nicolas and Cagiada, Matteo and Johansson, Kristoffer E and Boomsma, Wouter and Stein, Amelie and Lindorff-Larsen, Kresten},\n",
        "  journal={bioRxiv},\n",
        "  year={2022},\n",
        "  publisher={Cold Spring Harbor Laboratory}\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1fb2HFDWC2Yu"
      }
    }
  ]
}