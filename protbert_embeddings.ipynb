{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacfinberg/Novozymes-Enzyme-Stability-Prediction/blob/richard-nonpytorch-develop/protbert_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRcmomNzmFaQ",
        "outputId": "31b6ad28-6d1e-463b-9b37-f11cb20e0d79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive; drive.mount('/content/drive')   # OK to enable, if your kaggle.json is stored in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip -q install --upgrade --force-reinstall --no-deps kaggle > log  \n",
        "# !mkdir -p ~/.kaggle                                          \n",
        "# !cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json >log \n",
        "# !cp kaggle.json ~/.kaggle/kaggle.json > log              \n",
        "# !chmod 600 ~/.kaggle/kaggle.json                            \n",
        "# !kaggle config set -n competition -v novozymes-enzyme-stability-prediction   \n",
        "# !kaggle competitions download -c novozymes-enzyme-stability-prediction         \n",
        "# !unzip -o *.zip >> log                           \n",
        "# # !kaggle competitions leaderboard --show                "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0g4XjhLmk9i",
        "outputId": "74da8474-0f07-4b49-b6be-1e16471a84d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "- competition is now set to: novozymes-enzyme-stability-prediction\n",
            "novozymes-enzyme-stability-prediction.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# df = pd.read_csv('train.csv', low_memory=False); df"
      ],
      "metadata": {
        "id": "Me3nVTXsmpsC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_test = pd.read_csv('test.csv', low_memory=False)\n",
        "# df_test.head()"
      ],
      "metadata": {
        "id": "Q1vCptuf2H5g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read/Format Embeddings"
      ],
      "metadata": {
        "id": "sldwAh_r7cCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_train = pd.read_csv('/content/drive/Shareddrives/JHU Data Science Masters/Deep Learning/Project/embedding_csv.csv').set_index('seq_id')\n",
        "emb_test = pd.read_csv('/content/drive/Shareddrives/JHU Data Science Masters/Deep Learning/Project/test_embeddings.csv', engine='python', error_bad_lines=False).set_index('seq_id')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0rDX3NcyDyV",
        "outputId": "ec2a1a1e-a9fe-445e-c0ab-5cca48445af0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3326: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb_train['bert_embedding'] = emb_train['bert_embedding'].apply(lambda x: x.strip('][').split(', '))\n",
        "emb_test['bert_embedding'] = emb_test['bert_embedding'].apply(lambda x: x.strip('][').split(', '))\n",
        "emb_train = emb_train[['bert_embedding', 'tm']]\n",
        "emb_test = emb_test[['bert_embedding']]"
      ],
      "metadata": {
        "id": "8LHkjhS80tsy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, row in emb_train.iterrows():\n",
        "    emb_train.at[idx, 'bert_embedding'] = list(map(float, row['bert_embedding']))"
      ],
      "metadata": {
        "id": "RRLEcPKrdgIS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, row in emb_test.iterrows():\n",
        "    emb_test.at[idx, 'bert_embedding'] = list(map(float, row['bert_embedding']))"
      ],
      "metadata": {
        "id": "i7WO8eSpf_wY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_train['bert_embedding'][0]"
      ],
      "metadata": {
        "id": "9jdCA2mJezK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tree based regressor"
      ],
      "metadata": {
        "id": "nXEN_K8l7fr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Flatten, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "SEED = 0"
      ],
      "metadata": {
        "id": "Lpi1LR-v970J"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, test_X, train_y, test_y = train_test_split(np.array(emb_train['bert_embedding'].tolist()), np.array(emb_train['tm'].tolist()), test_size=1000, random_state=SEED, shuffle=True)"
      ],
      "metadata": {
        "id": "bc0jlf3X3PyQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor = GradientBoostingRegressor(loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', \n",
        "                                      min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, \n",
        "                                      init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, \n",
        "                                      validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
        "regressor = Hist\n",
        "regressor.fit(train_X, train_y)"
      ],
      "metadata": {
        "id": "KXmKHiRM7qjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm\n",
        "import lightgbm as lgb"
      ],
      "metadata": {
        "id": "sAHGNEmLj1zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as keras\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Normalization\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "tf.random.set_seed(0)   # always seed your experiments\n",
        "# Init = keras.initializers.RandomNormal(seed=0)\n",
        "Init= keras.initializers.LecunNormal(seed=0)\n",
        "import numpy as np\n",
        "epochs=100\n",
        "cosine_annealing_lr = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    0.001, 8*epochs)\n",
        "\n",
        "\n",
        "m = keras.models.Sequential([\n",
        "    Flatten(input_shape=[train_X.shape[1]]),\n",
        "    # Dropout(0.5, seed=0),\n",
        "    Dense(2048, activation=\"relu\", kernel_initializer=Init, name='Dense1'),\n",
        "    # Dropout(0.2, seed=0),\n",
        "    BatchNormalization(),\n",
        "    Dense(2048, activation=\"relu\", kernel_initializer=Init),\n",
        "    # Dropout(0.2, seed=0),\n",
        "    BatchNormalization(),\n",
        "    Dense(2048, activation=\"relu\", kernel_initializer=Init),\n",
        "    # Dropout(0.2, seed=0),\n",
        "    BatchNormalization(),\n",
        "    Dense(2048, activation=\"relu\", kernel_initializer=Init),\n",
        "    # Dropout(0.2, seed=0),\n",
        "    BatchNormalization(),\n",
        "    Dense(2048, activation=\"relu\", kernel_initializer=Init),\n",
        "    # Dropout(0.2, seed=0),\n",
        "    BatchNormalization(),\n",
        "    Dense(1, kernel_initializer=Init)])\n",
        "m.summary()\n",
        "m.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(\n",
        "                # learning_rate = cosine_annealing_lr\n",
        "                learning_rate = 0.001\n",
        "                ), metrics=['mse'])\n",
        "hist = m.fit(train_X,\n",
        "             train_y, \n",
        "             batch_size=2096, \n",
        "             epochs=100, \n",
        "             validation_split=0.2, \n",
        "            #  callbacks=[wandb.keras.WandbCallback()]\n",
        "             )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IWT9MVfj6_3",
        "outputId": "4ccea956-35c3-400b-e97a-50b4852d67ba"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_5 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " Dense1 (Dense)              (None, 2048)              2099200   \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 2048)             8192      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 2048)              4196352   \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 2048)             8192      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 2048)              4196352   \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 2048)             8192      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 2048)              4196352   \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 2048)             8192      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 2048)              4196352   \n",
            "                                                                 \n",
            " batch_normalization_21 (Bat  (None, 2048)             8192      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 1)                 2049      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,927,617\n",
            "Trainable params: 18,907,137\n",
            "Non-trainable params: 20,480\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 3s 112ms/step - loss: 2615.2146 - mse: 2615.2146 - val_loss: 6759.1089 - val_mse: 6759.1089\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 2546.9600 - mse: 2546.9600 - val_loss: 3632.2261 - val_mse: 3632.2261\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 1s 91ms/step - loss: 2481.6528 - mse: 2481.6528 - val_loss: 2813.0173 - val_mse: 2813.0173\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 1s 89ms/step - loss: 2362.3147 - mse: 2362.3147 - val_loss: 870.6871 - val_mse: 870.6871\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 2163.0703 - mse: 2163.0703 - val_loss: 772.8118 - val_mse: 772.8118\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 1875.8174 - mse: 1875.8174 - val_loss: 865.0514 - val_mse: 865.0514\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 1515.0955 - mse: 1515.0955 - val_loss: 510.8757 - val_mse: 510.8757\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 1125.5756 - mse: 1125.5756 - val_loss: 836.4771 - val_mse: 836.4771\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 763.8469 - mse: 763.8469 - val_loss: 291.8382 - val_mse: 291.8382\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 481.7076 - mse: 481.7076 - val_loss: 284.6549 - val_mse: 284.6549\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 304.9285 - mse: 304.9285 - val_loss: 233.6661 - val_mse: 233.6661\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 221.8948 - mse: 221.8948 - val_loss: 232.8345 - val_mse: 232.8345\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 199.1368 - mse: 199.1368 - val_loss: 242.9050 - val_mse: 242.9050\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 1s 82ms/step - loss: 195.0131 - mse: 195.0131 - val_loss: 277.0359 - val_mse: 277.0359\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 194.7167 - mse: 194.7167 - val_loss: 259.9389 - val_mse: 259.9389\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 1s 89ms/step - loss: 194.3843 - mse: 194.3843 - val_loss: 231.3240 - val_mse: 231.3240\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 194.1860 - mse: 194.1860 - val_loss: 232.5756 - val_mse: 232.5756\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 193.6945 - mse: 193.6945 - val_loss: 217.9153 - val_mse: 217.9153\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 193.7333 - mse: 193.7333 - val_loss: 209.9700 - val_mse: 209.9700\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 194.0270 - mse: 194.0270 - val_loss: 206.6093 - val_mse: 206.6093\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 193.8219 - mse: 193.8219 - val_loss: 203.8128 - val_mse: 203.8128\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 1s 89ms/step - loss: 193.6824 - mse: 193.6824 - val_loss: 208.3994 - val_mse: 208.3994\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 1s 82ms/step - loss: 194.2797 - mse: 194.2797 - val_loss: 204.2164 - val_mse: 204.2164\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 1s 82ms/step - loss: 193.9059 - mse: 193.9059 - val_loss: 202.8314 - val_mse: 202.8314\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 1s 89ms/step - loss: 193.5254 - mse: 193.5254 - val_loss: 199.6606 - val_mse: 199.6606\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 1s 82ms/step - loss: 193.5531 - mse: 193.5531 - val_loss: 199.0740 - val_mse: 199.0740\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 193.3390 - mse: 193.3390 - val_loss: 198.0348 - val_mse: 198.0348\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 194.2649 - mse: 194.2649 - val_loss: 200.6409 - val_mse: 200.6409\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 1s 89ms/step - loss: 194.6077 - mse: 194.6077 - val_loss: 199.8775 - val_mse: 199.8775\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 193.8513 - mse: 193.8513 - val_loss: 199.3110 - val_mse: 199.3110\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 1s 82ms/step - loss: 193.7780 - mse: 193.7780 - val_loss: 200.3672 - val_mse: 200.3672\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 193.7253 - mse: 193.7253 - val_loss: 199.5343 - val_mse: 199.5343\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 1s 89ms/step - loss: 193.7346 - mse: 193.7346 - val_loss: 197.4446 - val_mse: 197.4446\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 1s 89ms/step - loss: 193.8446 - mse: 193.8446 - val_loss: 199.1746 - val_mse: 199.1746\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 1s 89ms/step - loss: 193.3860 - mse: 193.3860 - val_loss: 196.5170 - val_mse: 196.5170\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 1s 89ms/step - loss: 193.1990 - mse: 193.1990 - val_loss: 198.6648 - val_mse: 198.6648\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 193.3242 - mse: 193.3242 - val_loss: 198.7351 - val_mse: 198.7351\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 1s 89ms/step - loss: 193.7992 - mse: 193.7992 - val_loss: 199.1085 - val_mse: 199.1085\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 193.7864 - mse: 193.7864 - val_loss: 199.8858 - val_mse: 199.8858\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 194.4787 - mse: 194.4787 - val_loss: 200.2569 - val_mse: 200.2569\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 1s 89ms/step - loss: 193.6122 - mse: 193.6122 - val_loss: 198.6783 - val_mse: 198.6783\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 193.6898 - mse: 193.6898 - val_loss: 197.9193 - val_mse: 197.9193\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 193.6647 - mse: 193.6647 - val_loss: 197.8794 - val_mse: 197.8794\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 1s 89ms/step - loss: 193.3527 - mse: 193.3527 - val_loss: 197.2079 - val_mse: 197.2079\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 193.3246 - mse: 193.3246 - val_loss: 197.2399 - val_mse: 197.2399\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 193.4978 - mse: 193.4978 - val_loss: 196.4780 - val_mse: 196.4780\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 193.4109 - mse: 193.4109 - val_loss: 197.0467 - val_mse: 197.0467\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 1s 91ms/step - loss: 193.7750 - mse: 193.7750 - val_loss: 197.0563 - val_mse: 197.0563\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 193.4375 - mse: 193.4375 - val_loss: 196.6456 - val_mse: 196.6456\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 193.5600 - mse: 193.5600 - val_loss: 196.6584 - val_mse: 196.6584\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 1s 91ms/step - loss: 193.1332 - mse: 193.1332 - val_loss: 197.0281 - val_mse: 197.0281\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 193.3903 - mse: 193.3903 - val_loss: 197.1735 - val_mse: 197.1735\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 1s 91ms/step - loss: 193.6844 - mse: 193.6844 - val_loss: 196.3001 - val_mse: 196.3001\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 193.1249 - mse: 193.1249 - val_loss: 196.1828 - val_mse: 196.1828\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 193.3345 - mse: 193.3345 - val_loss: 197.5583 - val_mse: 197.5583\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 193.4003 - mse: 193.4003 - val_loss: 197.8143 - val_mse: 197.8143\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 1s 91ms/step - loss: 192.8132 - mse: 192.8132 - val_loss: 195.8715 - val_mse: 195.8715\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 193.4057 - mse: 193.4057 - val_loss: 196.2906 - val_mse: 196.2906\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 1s 91ms/step - loss: 193.2071 - mse: 193.2071 - val_loss: 196.9707 - val_mse: 196.9707\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 193.2332 - mse: 193.2332 - val_loss: 196.1886 - val_mse: 196.1886\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 1s 91ms/step - loss: 193.2268 - mse: 193.2268 - val_loss: 196.4696 - val_mse: 196.4696\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 193.3830 - mse: 193.3830 - val_loss: 198.2561 - val_mse: 198.2561\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 1s 91ms/step - loss: 193.3552 - mse: 193.3552 - val_loss: 198.4919 - val_mse: 198.4919\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 1s 91ms/step - loss: 193.6163 - mse: 193.6163 - val_loss: 195.9876 - val_mse: 195.9876\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 1s 91ms/step - loss: 192.9766 - mse: 192.9766 - val_loss: 198.0052 - val_mse: 198.0052\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 192.9948 - mse: 192.9948 - val_loss: 197.5946 - val_mse: 197.5946\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 192.8457 - mse: 192.8457 - val_loss: 197.4869 - val_mse: 197.4869\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 1s 91ms/step - loss: 193.0835 - mse: 193.0835 - val_loss: 197.5836 - val_mse: 197.5836\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 193.4137 - mse: 193.4137 - val_loss: 197.4935 - val_mse: 197.4935\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 1s 85ms/step - loss: 193.1944 - mse: 193.1944 - val_loss: 197.4691 - val_mse: 197.4691\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 193.2899 - mse: 193.2899 - val_loss: 196.7372 - val_mse: 196.7372\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 1s 91ms/step - loss: 193.1020 - mse: 193.1020 - val_loss: 196.4705 - val_mse: 196.4705\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 193.3942 - mse: 193.3942 - val_loss: 198.3599 - val_mse: 198.3599\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 193.1490 - mse: 193.1490 - val_loss: 198.5193 - val_mse: 198.5193\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 1s 91ms/step - loss: 193.4526 - mse: 193.4526 - val_loss: 197.0663 - val_mse: 197.0663\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 193.2916 - mse: 193.2916 - val_loss: 197.5509 - val_mse: 197.5509\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 192.9484 - mse: 192.9484 - val_loss: 196.8761 - val_mse: 196.8761\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 1s 91ms/step - loss: 193.0068 - mse: 193.0068 - val_loss: 198.0147 - val_mse: 198.0147\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 1s 92ms/step - loss: 193.4051 - mse: 193.4051 - val_loss: 198.6485 - val_mse: 198.6485\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 193.5981 - mse: 193.5981 - val_loss: 197.3182 - val_mse: 197.3182\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 1s 91ms/step - loss: 193.4564 - mse: 193.4564 - val_loss: 197.0925 - val_mse: 197.0925\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 193.7020 - mse: 193.7020 - val_loss: 196.5981 - val_mse: 196.5981\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 193.3967 - mse: 193.3967 - val_loss: 196.2825 - val_mse: 196.2825\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 194.0840 - mse: 194.0840 - val_loss: 196.4178 - val_mse: 196.4178\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 193.7641 - mse: 193.7641 - val_loss: 196.3595 - val_mse: 196.3595\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 194.0086 - mse: 194.0086 - val_loss: 195.8256 - val_mse: 195.8256\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 1s 91ms/step - loss: 193.4250 - mse: 193.4250 - val_loss: 196.5028 - val_mse: 196.5028\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 193.6089 - mse: 193.6089 - val_loss: 197.4341 - val_mse: 197.4341\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 193.6045 - mse: 193.6045 - val_loss: 197.7101 - val_mse: 197.7101\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 192.9168 - mse: 192.9168 - val_loss: 198.3333 - val_mse: 198.3333\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 193.3819 - mse: 193.3819 - val_loss: 197.0027 - val_mse: 197.0027\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 192.9427 - mse: 192.9427 - val_loss: 197.6987 - val_mse: 197.6987\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 1s 89ms/step - loss: 192.9316 - mse: 192.9316 - val_loss: 195.8547 - val_mse: 195.8547\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 193.1818 - mse: 193.1818 - val_loss: 196.3295 - val_mse: 196.3295\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 1s 89ms/step - loss: 193.8424 - mse: 193.8424 - val_loss: 196.4353 - val_mse: 196.4353\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 193.3507 - mse: 193.3507 - val_loss: 197.0111 - val_mse: 197.0111\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 1s 83ms/step - loss: 193.2633 - mse: 193.2634 - val_loss: 196.3441 - val_mse: 196.3441\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 193.3205 - mse: 193.3205 - val_loss: 197.3541 - val_mse: 197.3541\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 193.9758 - mse: 193.9758 - val_loss: 198.2158 - val_mse: 198.2158\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 193.4447 - mse: 193.4447 - val_loss: 197.5035 - val_mse: 197.5035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regressor2 = XGBRegressor()\n",
        "regressor2.fit(train_X, train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRnEFTnhCaib",
        "outputId": "c4603dbe-9133-4017-f8c8-c43cb6feb89e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06:34:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor()"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preds = regressor2.predict(test_X)\n",
        "preds = m.predict(test_X)\n",
        "spearman = stats.spearmanr(test_y, preds)\n",
        "print(spearman)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sD3jRldV84PF",
        "outputId": "4f254539-6e66-41f6-ab25-0d69b76ba01f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 2ms/step\n",
            "SpearmanrResult(correlation=0.03038746665891997, pvalue=0.33707741457558815)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.hist(preds)\n",
        "plt.hist(test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "VcM5TRWd8YFh",
        "outputId": "65485559-1fad-45f8-9d3f-634ba9e238db"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  1.,  29.,  60., 335., 395.,  87.,  49.,  39.,   4.,   1.]),\n",
              " array([  0. ,  11.6,  23.2,  34.8,  46.4,  58. ,  69.6,  81.2,  92.8,\n",
              "        104.4, 116. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_final = trans_seq.transform(df_test['protein_sequence'])\n",
        "# test_final = projector.transform(test_final)\n",
        "# # test_final = scaler.transform(test_final)"
      ],
      "metadata": {
        "id": "Uoa8lv00A8MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_final = trans_seq.transform(df_test['protein_sequence'])\n",
        "# test_final = projector.transform(test_final)\n",
        "# test_final = scaler.transform(test_final)\n",
        "# predictions = model.predict(test_final)\n",
        "\n",
        "# ToCSV = lambda df_tmp, fname: df_tmp.to_csv(f'{fname}.csv', index_label='seq_id')\n",
        "# pY = pd.DataFrame(predictions, index=range(31390,len(predictions)+31390), columns=['tm'])\n",
        "# ToCSV(pY, './MySubmission')"
      ],
      "metadata": {
        "id": "4zhwrBQZr624",
        "outputId": "1cacca16-d764-4316-a72c-606bd86fac29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76/76 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = m.predict(np.array(emb_test['bert_embedding'].tolist()))\n",
        "pY = pd.DataFrame(predictions, index=range(31390,len(predictions)+31390), columns=['tm'])\n",
        "ToCSV = lambda df, fname: df.round(2).to_csv(f'{fname}.csv', index_label='id') # rounds values to 2 decimals\n",
        "ToCSV(pY, './MySubmission')"
      ],
      "metadata": {
        "id": "KkqsSxAzm64k",
        "outputId": "329489e1-bb92-4189-92e5-336833385101",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76/76 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9eCah4xznIe6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}