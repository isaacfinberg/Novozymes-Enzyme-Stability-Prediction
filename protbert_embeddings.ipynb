{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacfinberg/Novozymes-Enzyme-Stability-Prediction/blob/richard-nonpytorch-develop/protbert_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRcmomNzmFaQ",
        "outputId": "31b6ad28-6d1e-463b-9b37-f11cb20e0d79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive; drive.mount('/content/drive')   # OK to enable, if your kaggle.json is stored in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip -q install --upgrade --force-reinstall --no-deps kaggle > log  \n",
        "# !mkdir -p ~/.kaggle                                          \n",
        "# !cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json >log \n",
        "# !cp kaggle.json ~/.kaggle/kaggle.json > log              \n",
        "# !chmod 600 ~/.kaggle/kaggle.json                            \n",
        "# !kaggle config set -n competition -v novozymes-enzyme-stability-prediction   \n",
        "# !kaggle competitions download -c novozymes-enzyme-stability-prediction         \n",
        "# !unzip -o *.zip >> log                           \n",
        "# # !kaggle competitions leaderboard --show                "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0g4XjhLmk9i",
        "outputId": "74da8474-0f07-4b49-b6be-1e16471a84d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "- competition is now set to: novozymes-enzyme-stability-prediction\n",
            "novozymes-enzyme-stability-prediction.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# df = pd.read_csv('train.csv', low_memory=False); df"
      ],
      "metadata": {
        "id": "Me3nVTXsmpsC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_test = pd.read_csv('test.csv', low_memory=False)\n",
        "# df_test.head()"
      ],
      "metadata": {
        "id": "Q1vCptuf2H5g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read/Format Embeddings"
      ],
      "metadata": {
        "id": "sldwAh_r7cCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_train = pd.read_csv('/content/drive/Shareddrives/JHU Data Science Masters/Deep Learning/Project/embedding_csv.csv').set_index('seq_id')\n",
        "emb_test = pd.read_csv('/content/drive/Shareddrives/JHU Data Science Masters/Deep Learning/Project/test_embeddings.csv', engine='python', error_bad_lines=False).set_index('seq_id')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0rDX3NcyDyV",
        "outputId": "ec2a1a1e-a9fe-445e-c0ab-5cca48445af0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3326: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb_train['bert_embedding'] = emb_train['bert_embedding'].apply(lambda x: x.strip('][').split(', '))\n",
        "emb_test['bert_embedding'] = emb_test['bert_embedding'].apply(lambda x: x.strip('][').split(', '))\n",
        "emb_train = emb_train[['bert_embedding', 'tm']]\n",
        "emb_test = emb_test[['bert_embedding']]"
      ],
      "metadata": {
        "id": "8LHkjhS80tsy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, row in emb_train.iterrows():\n",
        "    emb_train.at[idx, 'bert_embedding'] = list(map(float, row['bert_embedding']))"
      ],
      "metadata": {
        "id": "RRLEcPKrdgIS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, row in emb_test.iterrows():\n",
        "    emb_test.at[idx, 'bert_embedding'] = list(map(float, row['bert_embedding']))"
      ],
      "metadata": {
        "id": "i7WO8eSpf_wY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_train['bert_embedding'][0]"
      ],
      "metadata": {
        "id": "9jdCA2mJezK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(data)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "stHUhuRh37br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tree based regressor"
      ],
      "metadata": {
        "id": "nXEN_K8l7fr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Flatten, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "SEED = 0"
      ],
      "metadata": {
        "id": "Lpi1LR-v970J"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, test_X, train_y, test_y = train_test_split(np.array(emb_train['bert_embedding'].tolist()), np.array(emb_train['tm'].tolist()), test_size=1000, random_state=SEED, shuffle=True)"
      ],
      "metadata": {
        "id": "bc0jlf3X3PyQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor = GradientBoostingRegressor(loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', \n",
        "                                      min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, \n",
        "                                      init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, \n",
        "                                      validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
        "regressor = Hist\n",
        "regressor.fit(train_X, train_y)"
      ],
      "metadata": {
        "id": "KXmKHiRM7qjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm\n",
        "import lightgbm as lgb"
      ],
      "metadata": {
        "id": "sAHGNEmLj1zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as keras\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Normalization\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "tf.random.set_seed(0)   # always seed your experiments\n",
        "# Init = keras.initializers.RandomNormal(seed=0)\n",
        "Init= keras.initializers.LecunNormal(seed=0)\n",
        "import numpy as np\n",
        "epochs=100\n",
        "cosine_annealing_lr = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    0.001, 8*epochs)\n",
        "\n",
        "\n",
        "m = keras.models.Sequential([\n",
        "    Flatten(input_shape=[train_X.shape[1]]),\n",
        "    # Dropout(0.5, seed=0),\n",
        "    Dense(1024, activation=\"relu\", kernel_initializer=Init, name='Dense1'),\n",
        "    # Dropout(0.2, seed=0),\n",
        "    BatchNormalization(),\n",
        "    Dense(1024, activation=\"relu\", kernel_initializer=Init),\n",
        "    # Dropout(0.2, seed=0),\n",
        "    BatchNormalization(),\n",
        "    Dense(1024, activation=\"relu\", kernel_initializer=Init),\n",
        "    # Dropout(0.2, seed=0),\n",
        "    BatchNormalization(),\n",
        "    Dense(1, kernel_initializer=Init)])\n",
        "m.summary()\n",
        "m.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(\n",
        "                # learning_rate = cosine_annealing_lr\n",
        "                learning_rate = 0.001\n",
        "                ), metrics=['mse'])\n",
        "hist = m.fit(train_X,\n",
        "             train_y, \n",
        "             batch_size=2096, \n",
        "             epochs=100, \n",
        "             validation_split=0.2, \n",
        "            #  callbacks=[wandb.keras.WandbCallback()]\n",
        "             )"
      ],
      "metadata": {
        "id": "6IWT9MVfj6_3",
        "outputId": "1c949549-0ffa-4b5f-da4f-17c4fbfd95cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_3 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " Dense1 (Dense)              (None, 1024)              1049600   \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1024)              1049600   \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 1024)             4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 1024)             4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 1025      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,162,113\n",
            "Trainable params: 3,155,969\n",
            "Non-trainable params: 6,144\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 1s 47ms/step - loss: 2597.3232 - mse: 2597.3232 - val_loss: 2684.8877 - val_mse: 2684.8877\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 2555.3901 - mse: 2555.3901 - val_loss: 2416.0071 - val_mse: 2416.0071\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2493.5391 - mse: 2493.5391 - val_loss: 2255.2234 - val_mse: 2255.2234\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 2390.1160 - mse: 2390.1160 - val_loss: 2232.1365 - val_mse: 2232.1365\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 2236.0742 - mse: 2236.0742 - val_loss: 1854.5259 - val_mse: 1854.5259\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 2030.7179 - mse: 2030.7179 - val_loss: 1409.5591 - val_mse: 1409.5591\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 1781.5924 - mse: 1781.5924 - val_loss: 1251.9774 - val_mse: 1251.9774\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 1503.5166 - mse: 1503.5165 - val_loss: 916.8401 - val_mse: 916.8401\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 1216.5625 - mse: 1216.5625 - val_loss: 587.2999 - val_mse: 587.2999\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 943.1852 - mse: 943.1852 - val_loss: 469.5095 - val_mse: 469.5095\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 700.4052 - mse: 700.4052 - val_loss: 370.7674 - val_mse: 370.7674\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 507.6731 - mse: 507.6731 - val_loss: 267.5325 - val_mse: 267.5325\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 370.4856 - mse: 370.4856 - val_loss: 242.8781 - val_mse: 242.8781\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 279.1382 - mse: 279.1382 - val_loss: 238.8934 - val_mse: 238.8934\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 230.1634 - mse: 230.1634 - val_loss: 215.1467 - val_mse: 215.1467\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 206.2558 - mse: 206.2558 - val_loss: 204.5151 - val_mse: 204.5151\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 196.7201 - mse: 196.7201 - val_loss: 215.9526 - val_mse: 215.9526\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.8298 - mse: 193.8298 - val_loss: 207.9622 - val_mse: 207.9622\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 194.4651 - mse: 194.4651 - val_loss: 232.1006 - val_mse: 232.1006\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 195.1694 - mse: 195.1694 - val_loss: 215.5856 - val_mse: 215.5856\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 193.7314 - mse: 193.7314 - val_loss: 205.3645 - val_mse: 205.3645\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 193.3541 - mse: 193.3541 - val_loss: 205.1550 - val_mse: 205.1550\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.6233 - mse: 193.6233 - val_loss: 208.0838 - val_mse: 208.0838\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 194.1053 - mse: 194.1053 - val_loss: 204.8268 - val_mse: 204.8268\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 193.3716 - mse: 193.3716 - val_loss: 203.4119 - val_mse: 203.4119\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 193.3810 - mse: 193.3810 - val_loss: 203.2677 - val_mse: 203.2677\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 193.0586 - mse: 193.0586 - val_loss: 201.7783 - val_mse: 201.7783\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 194.3096 - mse: 194.3096 - val_loss: 202.4888 - val_mse: 202.4888\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 194.9385 - mse: 194.9385 - val_loss: 199.5161 - val_mse: 199.5161\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 194.5546 - mse: 194.5546 - val_loss: 202.2890 - val_mse: 202.2890\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.5671 - mse: 193.5671 - val_loss: 201.0458 - val_mse: 201.0458\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.5189 - mse: 193.5189 - val_loss: 201.2394 - val_mse: 201.2394\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.9008 - mse: 193.9008 - val_loss: 199.7338 - val_mse: 199.7338\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.6434 - mse: 193.6434 - val_loss: 198.8399 - val_mse: 198.8399\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 193.3928 - mse: 193.3928 - val_loss: 200.5748 - val_mse: 200.5748\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 192.7972 - mse: 192.7972 - val_loss: 199.2250 - val_mse: 199.2250\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 192.6703 - mse: 192.6703 - val_loss: 197.7800 - val_mse: 197.7800\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.7329 - mse: 193.7329 - val_loss: 197.3367 - val_mse: 197.3367\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 193.1369 - mse: 193.1369 - val_loss: 197.6699 - val_mse: 197.6699\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 194.6732 - mse: 194.6732 - val_loss: 196.6786 - val_mse: 196.6786\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 193.9532 - mse: 193.9532 - val_loss: 200.1888 - val_mse: 200.1888\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 194.1553 - mse: 194.1553 - val_loss: 196.6362 - val_mse: 196.6362\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 193.6671 - mse: 193.6671 - val_loss: 196.9769 - val_mse: 196.9769\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 193.0677 - mse: 193.0677 - val_loss: 195.6393 - val_mse: 195.6393\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.0301 - mse: 193.0301 - val_loss: 197.3697 - val_mse: 197.3697\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 192.8881 - mse: 192.8881 - val_loss: 195.7149 - val_mse: 195.7149\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.2274 - mse: 193.2274 - val_loss: 198.6290 - val_mse: 198.6290\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.3847 - mse: 193.3847 - val_loss: 195.7547 - val_mse: 195.7547\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 193.4010 - mse: 193.4010 - val_loss: 197.7702 - val_mse: 197.7702\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.2845 - mse: 193.2845 - val_loss: 196.0029 - val_mse: 196.0029\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 192.6826 - mse: 192.6826 - val_loss: 196.9430 - val_mse: 196.9430\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.0770 - mse: 193.0770 - val_loss: 196.3916 - val_mse: 196.3916\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 193.7151 - mse: 193.7151 - val_loss: 195.6470 - val_mse: 195.6470\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 193.8048 - mse: 193.8048 - val_loss: 197.3826 - val_mse: 197.3826\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.1958 - mse: 193.1958 - val_loss: 195.9145 - val_mse: 195.9145\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 193.9841 - mse: 193.9841 - val_loss: 196.2993 - val_mse: 196.2993\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 193.3942 - mse: 193.3942 - val_loss: 195.7668 - val_mse: 195.7668\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 192.9897 - mse: 192.9897 - val_loss: 198.0140 - val_mse: 198.0140\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 192.8100 - mse: 192.8100 - val_loss: 196.9075 - val_mse: 196.9075\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 192.8210 - mse: 192.8210 - val_loss: 196.7323 - val_mse: 196.7323\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 192.8722 - mse: 192.8722 - val_loss: 196.3914 - val_mse: 196.3914\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.4642 - mse: 193.4642 - val_loss: 195.9661 - val_mse: 195.9661\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 192.5325 - mse: 192.5325 - val_loss: 197.7909 - val_mse: 197.7909\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 193.3546 - mse: 193.3546 - val_loss: 195.7299 - val_mse: 195.7299\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 192.9467 - mse: 192.9467 - val_loss: 196.9296 - val_mse: 196.9296\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 193.5586 - mse: 193.5586 - val_loss: 196.4243 - val_mse: 196.4243\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.0119 - mse: 193.0119 - val_loss: 196.5345 - val_mse: 196.5345\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 193.0698 - mse: 193.0698 - val_loss: 197.1390 - val_mse: 197.1390\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.0999 - mse: 193.0999 - val_loss: 197.3098 - val_mse: 197.3098\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 193.4067 - mse: 193.4067 - val_loss: 197.1796 - val_mse: 197.1796\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 192.9145 - mse: 192.9145 - val_loss: 198.3038 - val_mse: 198.3038\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 193.1258 - mse: 193.1258 - val_loss: 197.2203 - val_mse: 197.2203\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 193.1507 - mse: 193.1507 - val_loss: 198.5548 - val_mse: 198.5548\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 192.7478 - mse: 192.7478 - val_loss: 198.1593 - val_mse: 198.1593\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 192.9844 - mse: 192.9844 - val_loss: 196.8453 - val_mse: 196.8453\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 193.3454 - mse: 193.3454 - val_loss: 195.6554 - val_mse: 195.6554\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.0916 - mse: 193.0916 - val_loss: 195.9685 - val_mse: 195.9685\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.0875 - mse: 193.0875 - val_loss: 200.0397 - val_mse: 200.0397\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 193.1807 - mse: 193.1807 - val_loss: 197.2544 - val_mse: 197.2544\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 193.1147 - mse: 193.1147 - val_loss: 199.5197 - val_mse: 199.5197\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 192.9794 - mse: 192.9794 - val_loss: 200.5899 - val_mse: 200.5899\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 193.1035 - mse: 193.1035 - val_loss: 199.4972 - val_mse: 199.4972\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 192.6014 - mse: 192.6014 - val_loss: 200.7460 - val_mse: 200.7460\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 193.1469 - mse: 193.1469 - val_loss: 199.7384 - val_mse: 199.7384\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.0923 - mse: 193.0923 - val_loss: 200.1773 - val_mse: 200.1773\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 193.2094 - mse: 193.2094 - val_loss: 198.5497 - val_mse: 198.5497\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.6282 - mse: 193.6283 - val_loss: 200.0668 - val_mse: 200.0668\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 193.6169 - mse: 193.6169 - val_loss: 196.3332 - val_mse: 196.3332\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.5536 - mse: 193.5536 - val_loss: 199.8425 - val_mse: 199.8425\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.3055 - mse: 193.3055 - val_loss: 198.8446 - val_mse: 198.8446\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 193.4445 - mse: 193.4445 - val_loss: 198.4993 - val_mse: 198.4993\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 193.5171 - mse: 193.5171 - val_loss: 197.7283 - val_mse: 197.7283\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 192.9088 - mse: 192.9088 - val_loss: 196.6869 - val_mse: 196.6869\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.0357 - mse: 193.0357 - val_loss: 198.0561 - val_mse: 198.0560\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 194.0119 - mse: 194.0119 - val_loss: 199.2235 - val_mse: 199.2235\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 192.8206 - mse: 192.8206 - val_loss: 197.5294 - val_mse: 197.5294\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 193.1301 - mse: 193.1301 - val_loss: 205.6129 - val_mse: 205.6129\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 193.2709 - mse: 193.2709 - val_loss: 201.1588 - val_mse: 201.1588\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 193.5473 - mse: 193.5473 - val_loss: 201.7077 - val_mse: 201.7077\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 193.1225 - mse: 193.1225 - val_loss: 199.9266 - val_mse: 199.9266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regressor2 = XGBRegressor()\n",
        "regressor2.fit(train_X, train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRnEFTnhCaib",
        "outputId": "c4603dbe-9133-4017-f8c8-c43cb6feb89e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06:34:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor()"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preds = regressor2.predict(test_X)\n",
        "preds = m.predict(test_X)\n",
        "spearman = stats.spearmanr(test_y, preds)\n",
        "print(spearman)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sD3jRldV84PF",
        "outputId": "4f254539-6e66-41f6-ab25-0d69b76ba01f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 2ms/step\n",
            "SpearmanrResult(correlation=0.03038746665891997, pvalue=0.33707741457558815)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.hist(preds)\n",
        "plt.hist(test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "VcM5TRWd8YFh",
        "outputId": "65485559-1fad-45f8-9d3f-634ba9e238db"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  1.,  29.,  60., 335., 395.,  87.,  49.,  39.,   4.,   1.]),\n",
              " array([  0. ,  11.6,  23.2,  34.8,  46.4,  58. ,  69.6,  81.2,  92.8,\n",
              "        104.4, 116. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_final = trans_seq.transform(df_test['protein_sequence'])\n",
        "# test_final = projector.transform(test_final)\n",
        "# # test_final = scaler.transform(test_final)"
      ],
      "metadata": {
        "id": "Uoa8lv00A8MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_final = trans_seq.transform(df_test['protein_sequence'])\n",
        "# test_final = projector.transform(test_final)\n",
        "# test_final = scaler.transform(test_final)\n",
        "# predictions = model.predict(test_final)\n",
        "\n",
        "# ToCSV = lambda df_tmp, fname: df_tmp.to_csv(f'{fname}.csv', index_label='seq_id')\n",
        "# pY = pd.DataFrame(predictions, index=range(31390,len(predictions)+31390), columns=['tm'])\n",
        "# ToCSV(pY, './MySubmission')"
      ],
      "metadata": {
        "id": "4zhwrBQZr624",
        "outputId": "1cacca16-d764-4316-a72c-606bd86fac29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76/76 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    }
  ]
}